{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBSA Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from cartoframes.auth import set_default_credentials\n",
    "from cartoframes import read_carto, to_carto\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging \n",
    "from shapely import geometry\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top-level logger object\n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers: logger.removeHandler(handler)\n",
    "# manually set level \n",
    "logger.setLevel(logging.INFO)\n",
    "# print to console\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download EBSAs using url list from OneDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy csv of the urls to each EBSA page in the CBD clearing house to data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pull EBSAs from OneDrive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/EBSA_url_list.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info('Pulling EBSAs from OneDrive')\n",
    "# copy the csv with the urls for the EBSA jsons\n",
    "raw_data_file = os.path.join(os.getenv(\"OCEANWATCH_DATA_DIR\"),'EBSA_url_list.csv')\n",
    "dest_dir = os.path.join(data_dir, os.path.basename(raw_data_file))\n",
    "shutil.copy(raw_data_file, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv with the urls for the EBSA jsons\n",
    "url_df = pd.read_csv(raw_data_file,encoding='latin-1')\n",
    "url_list = url_df['CHM Url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the webpage for each EBSA for the url to download the geojson and download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0431476e3b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# regex pattern for the finding a geojson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatch_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'geojson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# scrape the page for the geojson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "source": [
    "# regex pattern for the finding a geojson \n",
    "match_st = re.compile(r'geojson') \n",
    "for url in url_list:\n",
    "    # scrape the page for the geojson\n",
    "    r = requests.get(url)  \n",
    "    c = r.content \n",
    "    soup = BeautifulSoup(c)\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"geojson$\")}):\n",
    "        href  = link.get('href')\n",
    "        url = 'https://chm.cbd.int' + href\n",
    "        # path to the raw data \n",
    "        raw_data_file = os.path.join(data_dir, os.path.basename(url))\n",
    "        # download raw data\n",
    "        r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the EBSA polygons into one shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of the geojsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebsa_files = glob.glob(os.path.join(data_dir, '*geojson'))\n",
    "gdf_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through geojson files to read each as a geodataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`data/SIO_3_EBSA.geojson' not recognized as a supported file format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not readdata/SIO_3_EBSA.geojson\n"
     ]
    }
   ],
   "source": [
    "for file in ebsa_files:\n",
    "    try:\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdf_list.append(gdf)\n",
    "    except Exception:\n",
    "        print(\"Could not read\" + file)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length EBSA = 479\n",
      "                                         NAME  \\\n",
      "0    Wrangel-Herald Shallow and Ratmanov Gyre   \n",
      "0                 The Small Phyllophora Field   \n",
      "0                            Oman Arabian Sea   \n",
      "0                         Northeastern Honshu   \n",
      "0  Sandspit/Hawks Bay and Adjacent Backwaters   \n",
      "\n",
      "                                          Workshop  EBSA_ID GLOBAL_ID  \\\n",
      "0                                           Arctic       10    ARC_10   \n",
      "0                        Black Sea and Caspian Sea        6    BSCS_6   \n",
      "0  North-West Indian Ocean and Adjacent Gulf Areas       29   NWIO_29   \n",
      "0                                Seas of East Asia       35     EA_35   \n",
      "0  North-West Indian Ocean and Adjacent Gulf Areas       16   NWIO_16   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((180.00000 70.14896, 179.99272 ...  \n",
      "0  POLYGON ((33.16825 45.80095, 33.15300 45.90800...  \n",
      "0  POLYGON ((59.45319 22.62132, 59.47155 22.63917...  \n",
      "0  MULTIPOLYGON (((141.73138 39.01435, 141.73008 ...  \n",
      "0  POLYGON ((66.93233 24.82471, 66.92801 24.81228...  \n"
     ]
    }
   ],
   "source": [
    "# merge geopandas dataframes \n",
    "gdf_ebsa = gpd.GeoDataFrame(pd.concat(gdf_list))\n",
    "print (\"Length EBSA = \" + str(len(gdf_ebsa.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or several characters couldn't be converted correctly from UTF-8 to ISO-8859-1.  This warning will not be emitted anymore.\n"
     ]
    }
   ],
   "source": [
    "# save processed dataset to shapefile\n",
    "gdf_ebsa.to_file('merged_ebsa.shp',driver='ESRI Shapefile')\n",
    "gdf_ebsa = '/home/rthoms/Github/resource-watch/wri-projects/ocean-watch/processing-scripts/biodiversity-protection/EBSA/merged_ebsa.shp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissolve the EBSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebsa = '/home/rthoms/Github/resource-watch/wri-projects/ocean-watch/processing-scripts/biodiversity-protection/EBSA/merged_ebsa.shp'\n",
    "gdf_ebsa = gpd.read_file(ebsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a field that is the same across all EBSAs to use in the dissolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "print(\"Length merged EBSA gdf = \" + len(gdf_ebsa.index))\n",
    "gdf_ebsa['dissolve']= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dissolve field to flatten the entire dataset, avoiding any double counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length EBSA = 1\n"
     ]
    }
   ],
   "source": [
    "# dissolve\n",
    "gdf_ebsa = gdf_ebsa.dissolve('dissolve')\n",
    "print (\"Length dissolved EBSA gdf = \" + str(len(gdf_ebsa.index)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed dataset to shapefile\n",
    "gdf_ebsa['NAME'] = 'EBSA'\n",
    "gdf_ebsa['Workshop'] = 'EBSA'\n",
    "gdf_ebsa['EBSA_ID'] = 0\n",
    "gdf_ebsa['GLOBAL_ID'] = 0\n",
    "gdf_ebsa.to_file('dissolved_ebsa.shp',driver='ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5df9a4ef679db67a0f9a60f8b8eb235a4adefd809cf6c33ee52fa80bbe16a2b8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
