{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EBSA Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from cartoframes.auth import set_default_credentials\n",
    "from cartoframes import read_carto, to_carto\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging \n",
    "from shapely import geometry\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "import shutil"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up logging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# get top-level logger object\n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers: logger.removeHandler(handler)\n",
    "# manually set level \n",
    "logger.setLevel(logging.INFO)\n",
    "# print to console\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set data directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_dir = \"data\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download EBSAs using url list from OneDrive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy csv of the urls to each EBSA page in the CBD clearing house to data directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "logger.info('Pulling EBSAs from OneDrive')\n",
    "# copy the csv with the urls for the EBSA jsons\n",
    "raw_data_file = os.path.join(os.getenv(\"OCEANWATCH_DATA_DIR\"),'EBSA_url_list.csv')\n",
    "dest_dir = os.path.join(data_dir, os.path.basename(raw_data_file))\n",
    "shutil.copy(raw_data_file, dest_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Pull EBSAs from OneDrive\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data/EBSA_url_list.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the csv as a pandas dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# read in the csv with the urls for the EBSA jsons\n",
    "url_df = pd.read_csv(raw_data_file,encoding='latin-1')\n",
    "url_list = url_df['CHM Url']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scrape the webpage for each EBSA for the url to download the geojson and download it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# regex pattern for the finding a geojson \n",
    "match_st = re.compile(r'geojson') \n",
    "for url in url_list:\n",
    "    # scrape the page for the geojson\n",
    "    r = requests.get(url)  \n",
    "    c = r.content \n",
    "    soup = BeautifulSoup(c)\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"geojson$\")}):\n",
    "        href  = link.get('href')\n",
    "        url = 'https://chm.cbd.int' + href\n",
    "        # path to the raw data \n",
    "        raw_data_file = os.path.join(data_dir, os.path.basename(url))\n",
    "        # download raw data\n",
    "        r = requests.get(url)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0431476e3b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# regex pattern for the finding a geojson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatch_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'geojson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# scrape the page for the geojson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge the EBSA polygons into one shapefile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create list of the geojsons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "ebsa_files = glob.glob(os.path.join(data_dir, '*geojson'))\n",
    "gdf_list = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate through geojson files to read each as a geodataframe "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for file in ebsa_files:\n",
    "    try:\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdf_list.append(gdf)\n",
    "    except Exception:\n",
    "        print(\"Could not read\" + file)\n",
    "     "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`data/SIO_3_EBSA.geojson' not recognized as a supported file format.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Could not readdata/SIO_3_EBSA.geojson\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge the geodataframes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# merge geopandas dataframes \n",
    "gdf_ebsa = gpd.GeoDataFrame(pd.concat(gdf_list))\n",
    "print (\"Length EBSA = \" + str(len(gdf_ebsa.index)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length EBSA = 479\n",
      "                                         NAME  \\\n",
      "0    Wrangel-Herald Shallow and Ratmanov Gyre   \n",
      "0                 The Small Phyllophora Field   \n",
      "0                            Oman Arabian Sea   \n",
      "0                         Northeastern Honshu   \n",
      "0  Sandspit/Hawks Bay and Adjacent Backwaters   \n",
      "\n",
      "                                          Workshop  EBSA_ID GLOBAL_ID  \\\n",
      "0                                           Arctic       10    ARC_10   \n",
      "0                        Black Sea and Caspian Sea        6    BSCS_6   \n",
      "0  North-West Indian Ocean and Adjacent Gulf Areas       29   NWIO_29   \n",
      "0                                Seas of East Asia       35     EA_35   \n",
      "0  North-West Indian Ocean and Adjacent Gulf Areas       16   NWIO_16   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((180.00000 70.14896, 179.99272 ...  \n",
      "0  POLYGON ((33.16825 45.80095, 33.15300 45.90800...  \n",
      "0  POLYGON ((59.45319 22.62132, 59.47155 22.63917...  \n",
      "0  MULTIPOLYGON (((141.73138 39.01435, 141.73008 ...  \n",
      "0  POLYGON ((66.93233 24.82471, 66.92801 24.81228...  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# save processed dataset to shapefile\n",
    "gdf_ebsa.to_file('merged_ebsa.shp',driver='ESRI Shapefile')\n",
    "gdf_ebsa = '/home/rthoms/Github/resource-watch/wri-projects/ocean-watch/processing-scripts/biodiversity-protection/EBSA/merged_ebsa.shp'\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "One or several characters couldn't be converted correctly from UTF-8 to ISO-8859-1.  This warning will not be emitted anymore.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dissolve the EBSAs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "ebsa = '/home/rthoms/Github/resource-watch/wri-projects/ocean-watch/processing-scripts/biodiversity-protection/EBSA/merged_ebsa.shp'\n",
    "gdf_ebsa = gpd.read_file(ebsa)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a field that is the same across all EBSAs to use in the dissolve"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(\"Length merged EBSA gdf = \" + len(gdf_ebsa.index))\n",
    "gdf_ebsa['dissolve']= 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "271\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the dissolve field to flatten the entire dataset, avoiding any double counting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# dissolve\n",
    "gdf_ebsa = gdf_ebsa.dissolve('dissolve')\n",
    "print (\"Length dissolved EBSA gdf = \" + str(len(gdf_ebsa.index)))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length EBSA = 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# save processed dataset to shapefile\n",
    "gdf_ebsa['NAME'] = 'EBSA'\n",
    "gdf_ebsa['Workshop'] = 'EBSA'\n",
    "gdf_ebsa['EBSA_ID'] = 0\n",
    "gdf_ebsa['GLOBAL_ID'] = 0\n",
    "gdf_ebsa.to_file('dissolved_ebsa.shp',driver='ESRI Shapefile')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5df9a4ef679db67a0f9a60f8b8eb235a4adefd809cf6c33ee52fa80bbe16a2b8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}